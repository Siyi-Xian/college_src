{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW4.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"0qKPC8dLcZ3a","colab_type":"text"},"cell_type":"markdown","source":["# HW3"]},{"metadata":{"id":"WqODxAR6cdKO","colab_type":"text"},"cell_type":"markdown","source":["## 1.\n","![](http://mypage.iu.edu/~siyixian/B455/HW4/1_AND.png)\n","\n","As the image shown above, we can see the model has really perfect accuracy. For logic AND function, this decision tree can do all choice perfectly.\n","\n","![](http://mypage.iu.edu/~siyixian/B455/HW4/1_XOR.png)\n","\n","XOR is a model which accuracy can be 100%. But this model needs more decision we need to make. I assume it is a good model but not such efficiency."]},{"metadata":{"id":"NMMgftImcfN5","colab_type":"text"},"cell_type":"markdown","source":["## 2.\n","Jim:\n","\n","$Entropy(p) = -\\Sigma_i p_i\\cdot \\log_2p_i=-\\frac{1}{2}\\log_2\\frac{1}{2}=\\frac{1}{2}$\n","\n","Jane:\n","\n","$Entropy(p) = -\\Sigma_i p_i\\cdot \\log_2p_i=-\\frac{1}{4}\\log_2\\frac{1}{4}=\\frac{1}{2}$\n","\n","Sarah and Simon:\n","\n","$Entropy(p) = -\\Sigma_i p_i\\cdot \\log_2p_i=-\\frac{1}{8}\\log_2\\frac{1}{8}=\\frac{3}{8}$\n","\n","John:\n","\n","$Entropy(p) = -\\Sigma_i p_i\\cdot \\log_2p_i=-0\\log_20=0$\n","\n","We only need one time to ask for average because Jim has alreafy take $50\\%$ chance."]},{"metadata":{"id":"x7aSVsM3eXbZ","colab_type":"text"},"cell_type":"markdown","source":["## 3.\n","$E(D)=-\\frac{1}{4}\\cdot\\log_2\\frac{1}{4}-\\frac{3}{4}\\cdot\\log_2\\frac{3}{4}=0.81$\n","\n","$G(D, G) = E(D) - \\Sigma_g \\frac{|D_g|}{|D|}E(D_g)=0.81-(\\frac{4}{8}(-\\frac{2}{4}\\log_2\\frac{2}{4}-\\frac{2}{4}\\log_2\\frac{2}{4}) + \\frac{4}{8}(-1\\log_2 1))=0.81-(\\frac{1}{2}+0)=0.31$\n","\n","$G(D, S) = E(D) - \\Sigma_s\\frac{|D_s|}{|D|}E(D_s)=0.81-(\\frac{4}{8}(-\\frac{1}{4}\\log_2\\frac{1}{4}-\\frac{3}{4}\\log_2\\frac{3}{4}) + \\frac{4}{8}(-\\frac{1}{4}\\log_2\\frac{1}{4}-\\frac{3}{4}\\log_2\\frac{3}{4}))=0.81-(\\frac{1}{2}\\cdot 0.81+\\frac{1}{2}\\cdot 0.81)=0$\n","\n","$G(D, P) = E(D) - \\Sigma_p \\frac{|D_p|}{|D|}E(D_p)=0.81-(\\frac{5}{8}(-\\frac{2}{5}\\log_2\\frac{2}{5}-\\frac{3}{5}\\log_2\\frac{3}{5}) + \\frac{3}{8}(-\\frac{3}{3}\\log_2\\frac{3}{3}))=0.81-(\\frac{5}{8}\\cdot 0.97+\\frac{3}{8}\\cdot 0)=0.2$\n","\n","Thus, we choose 'Gender' as first feature. If 'Gender' is false, the result always 'Vodka'. Therefore, we only need to sepreate our branch for true part.\n","\n","$E(G)=-\\frac{2}{4}\\log_2\\frac{2}{4}-\\frac{2}{4}\\log_2\\frac{2}{4}=1$\n","\n","$G(G, S)=E(G)-\\Sigma_s \\frac{|G_s|}{|G|}E(G_s)=1-(\\frac{1}{4}(-1\\log_2 1)+\\frac{3}{4}(-\\frac{1}{3}\\log_2\\frac{1}{3}-\\frac{2}{3}\\log_2\\frac{2}{3}))=1-(0+0.69)=0.31$\n","\n","$G(G, P)=E(G)-\\Sigma_p \\frac{|G_p|}{|G|}E(G_p)=1-(\\frac{1}{2}(-1\\log_2 1)+\\frac{1}{2}(-1\\log_2 1))=1-(0+0.69)=1$\n","\n","Then, we choose 'Pub' as our second feature. For true in 'Pub', we alwasy get 'Beer' as out put.\n","\n","![](http://mypage.iu.edu/~siyixian/B455/HW4/3.png)\n","\n","As a result, a female student and did not go to pub last night will drink 'Vodka' tonight."]},{"metadata":{"id":"UE-8LfjDoVxU","colab_type":"text"},"cell_type":"markdown","source":["## 4."]},{"metadata":{"id":"0tNS47lDzzjl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"98e77194-37ef-446b-fa80-efe9d5ea09cc","executionInfo":{"status":"ok","timestamp":1554488686581,"user_tz":240,"elapsed":440,"user":{"displayName":"Siyi Xian","photoUrl":"","userId":"06336570214189142044"}}},"cell_type":"code","source":["import numpy as np\n","\n","def calc_info_gain(data,classes,feature):\n","    gain = 0\n","    nData = len(data)\n","    # List the values that feature can take \n","    values = []\n","    for datapoint in data:\n","        if datapoint[feature] not in values:\n","            values.append(datapoint[feature])\n","    featureCounts = np.zeros(len(values))\n","    entropy = np.zeros(len(values))\n","    valueIndex = 0\n","    # Find where those values appear in data[feature] and the corresponding class\n","    for value in values:\n","        dataIndex = 0\n","        newClasses = []\n","        for datapoint in data:\n","            if datapoint[feature]==value:\n","                featureCounts[valueIndex]+=1\n","                newClasses.append(classes[dataIndex])\n","            dataIndex += 1\n","        # Get the values in newClasses\n","        classValues = []\n","        for aclass in newClasses:\n","            if classValues.count(aclass)==0:\n","                classValues.append(aclass)\n","        classCounts = np.zeros(len(classValues)) \n","        classIndex = 0\n","        for classValue in classValues:\n","            for aclass in newClasses:\n","                if aclass == classValue:\n","                    classCounts[classIndex]+=1 \n","            classIndex += 1\n","        for classIndex in range(len(classValues)):\n","            entropy[valueIndex] += calc_entropy(float(classCounts[classIndex])/sum(classCounts))\n","#         We do not need to calculate entropy with wight anymore\n","#         gain += float(featureCounts[valueIndex])/nData * entropy[valueIndex]\n","        valueIndex += 1 \n","#     Only return sum of Gini value\n","    return sum(entropy)\n","\n","\n","# Instead of calculate entropy, return square of probability\n","def calc_entropy(p):\n","    return p * p\n","#     if p!=0:\n","#         return -p * np.log2(p)\n","#     else:\n","#         return 0\n","\n","def make_tree(data, classes, featureNames): \n","    # Various initialisations suppressed\n","    nData = len(data)\n","    nFeatures = len(data[0])\n","\n","    # List the possible classes\n","    newClasses = []\n","    for aclass in classes:\n","        if newClasses.count(aclass)==0:\n","            newClasses.append(aclass)\n","\n","    # Compute the default class (and total entropy)\n","    frequency = np.zeros(len(newClasses))\n","\n","#     Do not need total entropy anymore, the total value will be 1 instead.\n","    totalEntropy = 1\n","#     index = 0\n","#     for aclass in newClasses:\n","#         frequency[index] = classes.count(aclass)\n","#         totalEntropy += calc_entropy(float(frequency[index])/nData)\n","#         index += 1\n","\n","    default = classes[np.argmax(frequency)]\n","    \n","    if nData==0 or nFeatures == 0:\n","        # Have reached an empty branch\n","        return default\n","    elif classes.count(classes[0]) == nData:\n","        # Only 1 class remains\n","        return classes[0] \n","    else:\n","        # Choose which feature is best\n","        gain = np.zeros(nFeatures)\n","        for feature in range(nFeatures):\n","            g = calc_info_gain(data,classes,feature)\n","            gain[feature] = totalEntropy - g \n","        bestFeature = np.argmax(gain)\n","        tree = {featureNames[bestFeature]:{}} \n","        \n","        # List the values that bestFeature can take\n","        values = []\n","        for datapoint in data:\n","            if datapoint[feature] not in values:\n","                values.append(datapoint[bestFeature])\n","\n","        # Find the possible feature values\n","        for value in values:\n","            # Find the datapoints with each feature value\n","            newData = []\n","            newClasses = []\n","            index = 0\n","            \n","            for datapoint in data:\n","                if datapoint[bestFeature]==value:\n","                    if bestFeature==0:\n","                        datapoint = datapoint[1:] \n","                        newNames = featureNames[1:]\n","                    elif bestFeature==nFeatures: \n","                        datapoint = datapoint[:-1] \n","                        newNames = featureNames[:-1]\n","                    else:\n","                        datapoint = datapoint[:bestFeature] \n","                        datapoint.extend(datapoint[bestFeature+1:]) \n","                        newNames = featureNames[:bestFeature] \n","                        newNames.extend(featureNames[bestFeature+1:])\n","                    newData.append(datapoint)\n","                    newClasses.append(classes[index]) \n","                index += 1\n","            # Now recurse to the next level\n","            subtree = make_tree(newData,newClasses,newNames) \n","            # And on returning, add the subtree on to the tree \n","            tree[featureNames[bestFeature]][value] = subtree\n","        return tree\n","     \n","data = [[0, 0], [1, 0], [0, 1], [1, 1]]\n","classes = [0, 1, 1, 1]\n","names = ['x1', 'x2']\n","tree = make_tree(data, classes, names)\n","\n","tree"],"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'x1': {0: {'x2': {0: 0, 1: 1}}, 1: 1}}"]},"metadata":{"tags":[]},"execution_count":133}]},{"metadata":{"id":"zTX7-npIobAm","colab_type":"text"},"cell_type":"markdown","source":["## 5.\n","For binary classification, the result will only be 0 or 1 (True or False). Thus we have at lease $50%$ accuracy.\n","\n","For muilty class prediction, we can choose one class and then classify with rest. For the rest classes reapte this step till the last one. Assume we have $n$ classes, the accuracy will be at least $\\frac{1}{2^{n-1}}$ based one binary classification."]},{"metadata":{"id":"egx76ydFoc37","colab_type":"text"},"cell_type":"markdown","source":["## 6."]},{"metadata":{"id":"_QxPJsYD32OJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2628e7db-6aaa-47b7-8ca2-20b0d241ab82","executionInfo":{"status":"ok","timestamp":1554488430733,"user_tz":240,"elapsed":1514,"user":{"displayName":"Siyi Xian","photoUrl":"","userId":"06336570214189142044"}}},"cell_type":"code","source":["import numpy as np\n","\n","def calc_info_gain(data,classes,feature):\n","    gain = 0\n","    nData = len(data)\n","    # List the values that feature can take \n","    values = []\n","    for datapoint in data:\n","        if datapoint[feature] not in values:\n","            values.append(datapoint[feature])\n","    featureCounts = np.zeros(len(values))\n","    entropy = np.zeros(len(values))\n","    valueIndex = 0\n","    # Find where those values appear in data[feature] and the corresponding class\n","    for value in values:\n","        dataIndex = 0\n","        newClasses = []\n","        for datapoint in data:\n","            if datapoint[feature]==value:\n","                featureCounts[valueIndex]+=1\n","                newClasses.append(classes[dataIndex])\n","            dataIndex += 1\n","        # Get the values in newClasses\n","        classValues = []\n","        for aclass in newClasses:\n","            if classValues.count(aclass)==0:\n","                classValues.append(aclass)\n","        classCounts = np.zeros(len(classValues)) \n","        classIndex = 0\n","        for classValue in classValues:\n","            for aclass in newClasses:\n","                if aclass == classValue:\n","                    classCounts[classIndex]+=1 \n","            classIndex += 1\n","        for classIndex in range(len(classValues)):\n","            entropy[valueIndex] += calc_entropy(float(classCounts[classIndex])/sum(classCounts))\n","        gain += float(featureCounts[valueIndex])/nData * entropy[valueIndex]\n","        valueIndex += 1 \n","    return gain\n","  \n","def calc_entropy(p):\n","    if p!=0:\n","        return -p * np.log2(p)\n","    else:\n","        return 0\n","      \n","def make_tree(data, classes, featureNames,maxlevel=-1,level=0,forest=0): \n","    # Various initialisations suppressed\n","    nData = len(data)\n","    nFeatures = len(data[0])\n","\n","    # List the possible classes\n","    newClasses = []\n","    for aclass in classes:\n","        if newClasses.count(aclass)==0:\n","            newClasses.append(aclass)\n","\n","    # Compute the default class (and total entropy)\n","    frequency = np.zeros(len(newClasses))\n","\n","    totalEntropy = 0\n","    index = 0\n","    for aclass in newClasses:\n","        frequency[index] = classes.count(aclass)\n","        totalEntropy += calc_entropy(float(frequency[index])/nData)\n","        index += 1\n","\n","    default = classes[np.argmax(frequency)]\n","    \n","    if nData==0 or nFeatures == 0 or (maxlevel>=0 and level>maxlevel):\n","        # Have reached an empty branch\n","        return default\n","    elif classes.count(classes[0]) == nData:\n","        # Only 1 class remains\n","        return classes[0] \n","    else:\n","        # Choose which feature is best\n","        gain = np.zeros(nFeatures)\n","        featureSet = list(range(nFeatures))\n","        # print(featureSet)\n","        if forest != 0:\n","            np.random.shuffle(featureSet)\n","            featureSet = featureSet[0:forest]\n","        for feature in range(nFeatures):\n","            g = calc_info_gain(data,classes,feature)\n","            gain[feature] = totalEntropy - g \n","            \n","        bestFeature = np.argmax(gain)\n","        tree = {featureNames[bestFeature]:{}} \n","        \n","        # List the values that bestFeature can take\n","        values = []\n","        for datapoint in data:\n","            if datapoint[feature] not in values:\n","                values.append(datapoint[bestFeature])\n","\n","        # Find the possible feature values\n","        for value in values:\n","            # Find the datapoints with each feature value\n","            newData = []\n","            newClasses = []\n","            index = 0\n","            \n","            for datapoint in data:\n","                if datapoint[bestFeature]==value:\n","                    if bestFeature==0:\n","                        datapoint = datapoint[1:] \n","                        newNames = featureNames[1:]\n","                    elif bestFeature==nFeatures: \n","                        datapoint = datapoint[:-1] \n","                        newNames = featureNames[:-1]\n","                    else:\n","                        datapoint = datapoint[:bestFeature] \n","                        datapoint.extend(datapoint[bestFeature+1:]) \n","                        newNames = featureNames[:bestFeature] \n","                        newNames.extend(featureNames[bestFeature+1:])\n","                    newData.append(datapoint)\n","                    newClasses.append(classes[index]) \n","                index += 1\n","            # Now recurse to the next level\n","            subtree = make_tree(newData,newClasses,newNames,maxlevel,level+1,forest) \n","            # And on returning, add the subtree on to the tree \n","            tree[featureNames[bestFeature]][value] = subtree\n","        return tree\n","      \n","def classify(tree, datapoint, featureNames):\n","    if type(tree) == int:\n","        # Have reached a leaf\n","        return tree\n","    \n","    else:\n","        a = list(tree.keys())[0]\n","        for i in range(len(featureNames)):\n","            if featureNames[i]==a:\n","                break\n","        \n","#         t = tree[a][datapoint[i]]\n","#         return classify(t,datapoint,features)\n","        try:\n","            t = tree[a][datapoint[i]]\n","            return classify(t,datapoint,features)\n","        except:\n","            return None\n","\n","\n","# Code from Chapter 13 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n","# by Stephen Marsland (http://stephenmonika.net)\n","\n","# You are free to use, change, or redistribute the code in any way you wish for\n","# non-commercial purposes, but please maintain the name of the original author.\n","# This code comes with no warranty of any kind.\n","\n","# Stephen Marsland, 2014\n","\n","class randomforest:\n","    \n","    \"\"\"The random forest algorithm based on the decision tree of Chapter 6\"\"\"\n","    def __init__(self):\n","        pass\n","        \n","    def rf(self,data,targets,features,nTrees,nSamples,nFeatures,maxlevel=5):\n","    \n","        nPoints = np.shape(data)[0]\n","        nDim = np.shape(data)[1]\n","        self.nSamples = nSamples\n","        self.nTrees = nTrees\n","                 \n","        classifiers = []\n","   \n","        for i in range(nTrees):\n","            # print(i)\n","            # Compute bootstrap samples\n","            samplePoints = np.random.randint(0,nPoints,(nPoints,nSamples))\n","        \n","            for j in range(nSamples):\n","                sample = []\n","                sampleTarget = []\n","                for k in range(nPoints):\n","                    sample.append(data[samplePoints[k,j]])\n","                    sampleTarget.append(targets[samplePoints[k,j]])\n","            # Train classifiers\n","            classifiers.append(make_tree(sample,sampleTarget,features,maxlevel,forest=nFeatures))\n","        return classifiers\n","    \n","    def rfclass(self,classifiers,data,features):\n","        \n","        decision = []\n","        # Majority voting\n","        for j in range(len(data)):\n","            outputs = []\n","            #print data[j]\n","            for i in range(self.nTrees):\n","                out = classify(classifiers[i],data[j],features)\n","                if out is not None:\n","                    outputs.append(out)\n","            # List the possible outputs\n","            out = []\n","            for each in outputs:\n","                if out.count(each)==0:\n","                    out.append(each)\n","            frequency = np.zeros(len(out))\n","        \n","            index = 0\n","            if len(out)>0:\n","                for each in out:\n","                    frequency[index] = outputs.count(each)\n","                    index += 1\n","                decision.append(out[frequency.argmax()])\n","            else:\n","                decision.append(None)\n","        return decision\n","\n","      \n","from sklearn.model_selection import train_test_split\n","\n","url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n","data = np.loadtxt(url, delimiter=\",\", dtype=np.str)\n","\n","# divide data into X and y\n","X = data[:, 1:10]\n","y = data[:, 10]\n","# scale all X\n","y = y.astype(int)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","features = [\"Clump\",\"Size\",\"Shape\",\"Marginal\",\"Single\",\"Bare\",\"Bland\",\"Normal\",\"Mitoses\" ]\n","\n","randomforest = randomforest()\n","\n","forest = randomforest.rf(X_train.tolist(),y_train.tolist(),features,20,5,5)\n","\n","predict = randomforest.rfclass(forest, X_test.tolist(), features)\n","\n","correct = 0\n","for i in range(len(predict)):\n","    if predict[i] == y_test[i]:\n","        correct += 1\n","print(\"Accuracy\", correct / len(predict))"],"execution_count":132,"outputs":[{"output_type":"stream","text":["Accuracy 0.9357142857142857\n"],"name":"stdout"}]},{"metadata":{"id":"04rEK8cmu3En","colab_type":"text"},"cell_type":"markdown","source":["As the result shown above, we can get that using random forest, we'll get a worse result than Both SVM and MLP. But after traning the model pridect result will be much more faster than what we have in SVM and MLP. Although, some time random forest cannot predict a full result because the choosen of m. However, to keep running time low, we cannot increse M to a large number. Thus there still some posiibilty to get a None result which is comfusing even though just have a really low probability."]},{"metadata":{"id":"S6k-5RjUoeH_","colab_type":"text"},"cell_type":"markdown","source":["## 7.\n","\n","No. I believe that we have to keep pruning algorithm when builting random forest. The reason is that if we do not prune, the model might be overfitting. Also, some time it will not produce a good out put becasue overfitting havd the limit number of featrues. "]}]}